{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf354241",
   "metadata": {},
   "source": [
    "# 1. What is Data Reduction \n",
    "\n",
    "Purpose:\n",
    "\n",
    "Data reduction is about reducing the size of a dataset while keeping its important information intact. This is useful to:\n",
    "\n",
    "Improve computational efficiency\n",
    "\n",
    "Reduce storage requirements\n",
    "\n",
    "Simplify models\n",
    "\n",
    "Remove redundant or irrelevant data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a90aa",
   "metadata": {},
   "source": [
    "\n",
    "# Key Techniques of Data Reduction\n",
    "\n",
    "Dimensionality Reduction – reducing the number of features (columns)\n",
    "\n",
    "Numerical Aggregation / Binning – grouping continuous values into bins\n",
    "\n",
    "Sampling / Row Reduction – reducing the number of rows\n",
    "\n",
    "Feature Selection – keeping only the most important variables\n",
    "\n",
    "Removing Redundant Data – dropping duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52e3d4",
   "metadata": {},
   "source": [
    "# 1. Remove duplicates\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Sometimes datasets have duplicate rows or columns that are exact copies.\n",
    "\n",
    "Removing duplicates reduces dataset size without losing any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa18b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Car  Price\n",
      "0   A  15000\n",
      "1   B  20000\n",
      "2   A  15000\n",
      "3   C  22000\n",
      "4   B  20000\n",
      "  Car  Price\n",
      "0   A  15000\n",
      "1   B  20000\n",
      "3   C  22000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Car': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'Price': [15000, 20000, 15000, 22000, 20000]\n",
    "})\n",
    "\n",
    "print(df)\n",
    "# Remove duplicate rows\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(df_no_duplicates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964fb40",
   "metadata": {},
   "source": [
    "# 2. Feature Selection (Reducing Columns)\n",
    "\n",
    "Feature selection is all about keeping only what matters, whether by statistical methods, model-based evaluation, or built-in importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecbf5648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation with Price:\n",
      " Price    1.000000\n",
      "Age     -0.355371\n",
      "HP       0.897448\n",
      "Doors    0.644831\n",
      "Name: Price, dtype: float64\n",
      "RFE Selected Features: Index(['Age', 'Doors'], dtype='object')\n",
      "Feature Importances:\n",
      " HP       0.727486\n",
      "Doors    0.138459\n",
      "Age      0.134056\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'Price':[15000, 20000, 16000, 22000, 21000],\n",
    "    'Age':[5, 3, 4, 2, 6],\n",
    "    'HP':[100, 120, 110, 130, 115],\n",
    "    'Doors':[3, 5, 3, 5, 3]\n",
    "})\n",
    "\n",
    "X = df[['Age','HP','Doors']]\n",
    "y = df['Price']\n",
    "\n",
    "# 1️ Correlation filter\n",
    "corr = df.corr()['Price']\n",
    "print(\"Correlation with Price:\\n\", corr)\n",
    "\n",
    "# 2️ Wrapper method - RFE\n",
    "model = LinearRegression()\n",
    "rfe = RFE(model, n_features_to_select=2)\n",
    "rfe.fit(X, y)\n",
    "print(\"RFE Selected Features:\", X.columns[rfe.support_])\n",
    "\n",
    "# 3️ Embedded method - RandomForest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X, y)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "print(\"Feature Importances:\\n\", importances.sort_values(ascending=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7aa80",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction (Reducing Correlated Features)\n",
    "\n",
    "Purpose:\n",
    "\n",
    "If a dataset has many correlated numeric columns, PCA or other techniques can reduce them into fewer components.\n",
    "\n",
    "Keeps most information while reducing columns.\n",
    "\n",
    "Example using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b0f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-297.63488302   38.90152899]\n",
      " [  56.86271549  -27.63409389]\n",
      " [ -84.63691419  -29.31268408]\n",
      " [ 268.3636357    45.61588976]\n",
      " [  57.04544601  -27.57064078]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_features = pd.DataFrame({\n",
    "    'HP': [100, 110, 105, 120, 115],\n",
    "    'CC': [1400, 1600, 1500, 1800, 1600],\n",
    "    'Weight': [1200, 1500, 1400, 1600, 1500]\n",
    "})\n",
    "\n",
    "# PCA to reduce 3 features into 2 components\n",
    "pca = PCA(n_components=2)\n",
    "reduced_features = pca.fit_transform(df_features)\n",
    "print(reduced_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2be82",
   "metadata": {},
   "source": [
    "# 4. Numerical Aggregation / Binning\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Continuous variables (like age, price, or mileage) can be grouped into bins.\n",
    "\n",
    "Makes data simpler and reduces the number of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521e9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Price PriceCategory\n",
      "0  15000           Low\n",
      "1  20000        Medium\n",
      "2  16000           Low\n",
      "3  22000          High\n",
      "4  21000          High\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Price':[15000, 20000, 16000, 22000, 21000]})\n",
    "\n",
    "# Bin Price into categories\n",
    "df['PriceCategory'] = pd.cut(df['Price'], bins=[0,16000,20000,25000], labels=['Low','Medium','High'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c971efad",
   "metadata": {},
   "source": [
    "# 5. Sampling / Row Reduction\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Large datasets may have millions of rows.\n",
    "\n",
    "Sampling allows us to work with a representative subset, reducing memory usage and speeding up analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a481e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Car  Price\n",
      "1   B  20000\n",
      "4   E  21000\n",
      "2   C  16000\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Car':['A','B','C','D','E'], 'Price':[15000,20000,16000,22000,21000]})\n",
    "\n",
    "# Randomly sample 60% of rows\n",
    "df_sampled = df.sample(frac=0.6, random_state=42)\n",
    "print(df_sampled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
